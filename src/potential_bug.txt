1. the data type for mask (uint32_t v.s. size_t)
2. RELATION_PADDING -- depends on SWWCB size
3. inline function reference
4. TUPLE_PER_SWWCB should be the power of two
5. tuple key starts from 1
6. page faults in the beginning (the response time of the first run is larger)
7. flush and memory fence (e.g., task_queue_add_atomic)
8. skew_handling only applies to 2-pass partitioning
9. lp_join only applies to pkfk and zipfian workload
10. phj_ind/shr_uni can not run on zipfian/sparsity workloads
11. for phj_ind/shr_uni, scaling factors may need to be modified according to different radix bits. more specifically, phj_ind_uni can use 1.78 scaling_factor for up to 18 bits while phj_shr_uni can use 1.58 scaling_factor for up to 18 bits, actually, it should be at most 16 bits (a safe value) for both phj_ind/shr_uni
12. for phj_ind_uni, should be very careful with payload workloads. when SKEW_HANDLING is turned on, the IND_UNI_SCALING_FACTOR should be tuned from 2.1 to 3.5 (roughly estimated, could be higher). the reason probably is that the "avg_part_thr_scaling_offset" will get smaller once the payload size increases or BUILDPART_THREAD_NUM increases, which causes the lower bound overflow of size_t data type (see IN_DEBUG codes of commmit: e9a7c03a79002eadd32fbf41324bc67646203531 for more details). for other workloads, setting IND_UNI_SCALING_FACTOR 1.78 is safe (see note 11 of this txt file).
13. for heavily skew dataset, in phj_shr SKEW_HANDLING, maybe only one thread is doing skew_handling partitioning (link buffer number smaller than thread number), thus, the bit number should not be too large, or decrease the value of PART_BUFFER_T_CAPACITY (but may increase the runtime)
14. data already takes up lots of space of NVM, runtimes of different runs may exibit a certain level of variance.
15. phj_shr_uni_*, the lock implementation should use pthread_mutex_t for better performance (don't use my own one-byte lock and __sync_fetch_and_add for some hidden compilation/linking errors)